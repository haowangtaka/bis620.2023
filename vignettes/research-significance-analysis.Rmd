---
title: "Evaluating the Determinants and Evolution of Clinical Trial Outcome Significance"
output: rmarkdown::html_vignette
author: Qifan Zhang qz277 Hao Wang hw575
format:
  html:
    self-contained: true
vignette: >
  %\VignetteIndexEntry{research-significance-analysis}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Background and Motivation
In this project, our aim is two-fold: firstly, to explore the relationships between the significance of clinical trial outcomes and factors such as the countries where these trials are conducted and the number of facilities involved; and secondly, to investigate the evolution of clinical trial outcome significance over time.

Clinical trials are critical in shaping medical practices and policies and are the cornerstone for evaluating newly proposed medical methods. Conducting comprehensive clinical trials often requires ample funding and a broad sample base. Intuitively, one might hypothesize that more significant results often emerge from well-resourced hospitals or research centers in developed regions. To delve into this hypothesis and extend our inquiry, we utilize data from the Aggregate Analysis of ClinicalTrials.gov (AACT). This allows us to study not only how geographical locations and the scale of facilities might affect trial outcomes but also how the significance of these outcomes has evolved over time.

This extended scope of research is particularly pertinent in light of recent shifts in global health landscapes and the advent of new medical technologies and methodologies. By incorporating a time series analysis, we can identify trends and patterns in the data, offering a dynamic perspective on the impact of location and facility scale on clinical trial outcomes.

The insights derived from this comprehensive study could be instrumental for stakeholders in clinical research. Understanding the influence of location, collaboration level, and their changes over time—factors we consider as "surrogate" variables of research resources—empowers policy-makers to allocate funds and resources more effectively. This, in turn, ensures more reliable and generalizable results, leading to more informed regulatory decisions and improved health outcomes globally. Our study aims to contribute to this critical discourse by providing a nuanced and temporal understanding of factors influencing the efficacy of clinical trials.

# Research Question

In this project, our primary objective is to explore potential associations between the significance of clinical trial outcomes and various factors, including the countries where these trials are conducted and the number of facilities involved. To accomplish this, we have decided to employ a linear regression model as our analytical tool, using the p-values derived from each clinical trial as the response variable.

Furthermore, we aim to gain insights into the broader trends in the significance of clinical trial outcomes by conducting preliminary time series analysis. This will allow us to examine how the significance of these outcomes has evolved over time.

# Data Cleaning and Exploration

In the following analysis, we will use `countries`, `outcome_analyses` and `calculated_values` tables from AACT database. For convenience, we download them and save them as .rda files in our bis620.2023 package.

```{r setup}
library(bis620.2023)
library(lubridate)
library(dplyr)
library(ggplot2)
library(purrr)
library(tidyr)
library(patchwork)
library(xts)
```

## Data Inspection

The table `countries` has 654,096 rows and 4 columns. The `name` column indicates the countries where the trials are conducted and the `removed` column indicates whether the countries are from `removed_countries` table. In addition, there are 81 missing values in the `name` column. There are 227 distinct countries in total. Furthermore, the number of cross-countries clinical trials are 39,753.

```{r}
countries
```

```{r}
countries |> is.na() |> colSums()
```

```{r}
countries |> distinct(name) |> count()
```

```{r}
countries |> group_by(nct_id) |> summarise(n = n()) |> filter(n >= 2) |> count()
```

The table `outcome_analyses` has 256,592 rows and 22 columns. In our project, we mainly consider the `p_value` column. There are 45,622 missing values in the `name` column.

```{r}
outcome_analyses
```

```{r}
outcome_analyses |> select(p_value) |> is.na() |> colSums()
```

The table `calculated_values` has 467,212 rows and 19 columns. In our project, we mainly consider the `number_of_facilities`, `actual_duration`, `has_us_facility` columns. There are 50,012, 185,328, and 50012 missing values in the `number_of_facilities`, `actual_duration`, `has_us_facility` columns respectively. In addition, there are 166,148 trials with facilities from USA.

```{r}
calculated_values
```

```{r}
calculated_values |> select(number_of_facilities, actual_duration, has_us_facility) |> is.na() |> colSums()
```

```{r}
calculated_values |> filter(has_us_facility == TRUE) |> count()
```

## Data Cleaning

For `countries` table, we remove those rows with `removed == TRUE` or `name == NA`. Although this may bring some selection bias in our further analysis, it can reduce the variance of our parameter estimation. For `outcome_analyses` table, we remove those rows with `p_value == NA`. For `calculated_values`, we remove those rows with `number_of_facilities == NA`, `actual_duration == NA` or `has_us_facility == NA`. After cleaning, we have 619,635 valid rows for `countries`, 210,970 valid rows for `outcome_analyses` and 258,619 valid rows for `calculated_values`.

Next, we left join `number_of_facilities` and `countries` on `outcome_analyses`. We choose `outcome_analyses` as our target table because `p_value` is our response variable. In addition, we drop those countries with less than 10 clinical trials. After further cleaning the `NA` value, we finalize a dataset with 746,903 rows. Some clinical trials may have many facilities from different countries involved such as the NCT03989232. Thus the dataset has some duplicate `nct_id` at this stage.

We conduct two stage cleaning above because we want to reduce the memory size in table joining process.

```{r}
countries_processed <- countries |> filter(removed == FALSE & !is.na(name)) |> select(nct_id, name)
outcome_analyses_processed <- outcome_analyses |> filter(!is.na(p_value)) |> select(nct_id, p_value)
calculated_values_processed <- calculated_values |> filter(!is.na(number_of_facilities) & !is.na(actual_duration) & !is.na(has_us_facility)) |> select(nct_id, number_of_facilities, actual_duration, has_us_facility)
```

```{r}
countries_processed |> count()
outcome_analyses_processed |> count()
calculated_values_processed |> count()
```

```{r}
d <- outcome_analyses_processed |> left_join(countries_processed, by="nct_id") |> left_join(calculated_values_processed, by="nct_id") |> filter(!is.na(name) & !is.na(number_of_facilities) & !is.na(actual_duration) & !is.na(has_us_facility)) |> filter(p_value >= 0 & p_value <= 1) |> distinct()

valid_countries <- d |> select(nct_id, name) |> distinct() |> group_by(name) |> summarise(n = n()) |> filter(n > 10) |> select(name) |> as.vector() |> unlist()
d <- d |> filter(name %in% valid_countries)
d
```

## Data Visualization and Transformation

### Distribution of p values

First, we explore the distribution of `p_value`. This may indicate further data transformation of `p_value`. Note that the value of the `p_value` is bounded in [0, 1] and is skewed to 0. For simplicity, we take a logit transformation on `p_value`. We can see that after transformation, the shape of the p.d.f. looks more like a normal distribution.

```{r}
min_p <- d |> select(p_value) |> min()
max_p <- d |> select(p_value) |> max()
p1 <- d |> select(nct_id, p_value) |> distinct() |>
    ggplot(aes(x = p_value)) +
    geom_histogram(bins = 30) +
  scale_x_continuous(limits = c(min_p, max_p)) +
  theme_bw() +
  ggtitle("Distribution of p values") + 
  xlab("p value") + 
  ylab("Count")

d <- d |> mutate(p_value = log((p_value) / (1 - p_value + 0.0001) + 0.0001))
min_p <- d |> select(p_value) |> min()
max_p <- d |> select(p_value) |> max()
p2 <- d |> select(nct_id, p_value) |> distinct() |>
    ggplot(aes(x = p_value)) +
    geom_histogram(bins = 30) +
  scale_x_continuous(limits = c(min_p, max_p)) + 
  theme_bw() + 
  ggtitle("Distribution of p values (transformed)") + 
  xlab("transformed p values") + 
  ylab("Count")

p1 + p2
```

Furthermore, we compare the distribution of p values of the trials conducted in the top 3 countries with most number of trials. From the three density plots, we do not see any differences among their distributions of the p values.

```{r}
d |> select(nct_id, name) |> distinct() |> group_by(name) |> summarise(n = n()) |> arrange(desc(n))
```

```{r}
top_three <- c("United States", "Canada", "Germany")
p <- d |> filter(name %in% top_three) |> select(nct_id, p_value, name) |> distinct() |>
  ggplot(aes(x = name, y = p_value)) + 
  geom_violin() + 
  theme_bw() + 
  ggtitle("Distribution of p values") + 
  xlab("Countries") + 
  ylab("p values")
p
```

### Distribution of Number of Facilities

We explore the distribution of `number_of_facilities` in this section. The average value of it is 149.6227. Note that the histogram and box-plot indicate that there may be some outliers in our dataset. For further analysis, we use 3/4-quantile to truncate the `number_of_facilities`. After truncation, the average value of it is 16.94371.

```{r}
mean(d$number_of_facilities)
```

```{r}
min_v <- d |> select(number_of_facilities) |> min()
max_v <- d |> select(number_of_facilities) |> max()
p1 <- d |> select(nct_id, number_of_facilities) |> distinct() |>
    ggplot(aes(x = number_of_facilities)) +
    geom_histogram(bins = 30) +
  scale_x_continuous(limits = c(min_v, max_v)) +
  theme_bw() +
  ggtitle("Distribution of Number of Facilities") + 
  xlab("Number of Facilities") + 
  ylab("Count")

p2 <- d |> select(nct_id, number_of_facilities) |> distinct() |>
    ggplot(aes(x = number_of_facilities)) + 
    geom_boxplot() +
    theme_bw() + 
    ggtitle("Box-plot of Number of Facilities") + 
  xlab("Number of Facilities") + 
  theme(axis.text.y = element_blank(),
             axis.ticks.y = element_blank())
p1 + p2  
```

```{r}
tmp <- d |> select(nct_id, number_of_facilities) |> distinct()
quantile_value <- quantile(tmp$number_of_facilities, 0.75)
d <- d |> filter(number_of_facilities <= quantile_value)
min_v <- d |> select(number_of_facilities) |> min()
max_v <- d |> select(number_of_facilities) |> max()
p1 <- d |> select(nct_id, number_of_facilities) |> distinct() |>
    ggplot(aes(x = number_of_facilities)) +
    geom_histogram(bins = 30) +
  scale_x_continuous(limits = c(min_v, max_v)) +
  theme_bw() +
  ggtitle("Distribution of Number of Facilities (Truncated)") + 
  xlab("Number of Facilities") + 
  ylab("Count")

p2 <- d |> select(nct_id, number_of_facilities) |> distinct() |>
    ggplot(aes(x = number_of_facilities)) + 
    geom_boxplot() +
    theme_bw() + 
    ggtitle("Box-plot of Number of Facilities (Truncated)") + 
  xlab("Number of Facilities") + 
  theme(axis.text.y = element_blank(),
             axis.ticks.y = element_blank())
p1 + p2  
```

```{r}
mean(d$number_of_facilities)
```


### Distribution of Actual Duration

Similar to the `number_of_facilities`, we visualize the distribution of `actual_duration`. The average value of it is 28.58358. Note that for `actual_duration`, all values look within a reasonable range.

```{r}
mean(d$actual_duration)
```

```{r}
min_v <- d |> select(actual_duration) |> min()
max_v <- d |> select(actual_duration) |> max()
p1 <- d |> select(nct_id, actual_duration) |> distinct() |>
    ggplot(aes(x = actual_duration)) +
    geom_histogram(bins = 30) +
  scale_x_continuous(limits = c(min_v, max_v)) +
  theme_bw() +
  ggtitle("Distribution of Actual Duration") + 
  xlab("Actual Duration") + 
  ylab("Count")

p2 <- d |> select(nct_id, actual_duration) |> distinct() |>
    ggplot(aes(x = actual_duration)) +
    geom_boxplot() +
    theme_bw() +
    ggtitle("Box-plot of Actual Duration") +
  xlab("Actual Duration") +
  theme(axis.text.y = element_blank(),
             axis.ticks.y = element_blank())
p1 + p2

```

# Analysis

## Full model

In this part, we first fit the `p_value` on all our candidate variables and conduct t-test for each variable. First, we generate the corresponding dummy variables and transform our dataset `d` to the one without duplicate `nct_id`.

```{r}
dummies_vars <- as_tibble(model.matrix(~ name - 1, d))
d_tmp <- bind_cols(d |> select(nct_id), dummies_vars) |> group_by(nct_id) |> summarise(across(where(is.numeric), sum, na.rm = TRUE))
d <- d |> select(-name) |> distinct() |> group_by(nct_id) |> summarise(across(where(is.numeric), mean, na.rm = TRUE))
d <- d |> inner_join(d_tmp, by = "nct_id")
d
```

Next, we fit our full model. Given confidence level 95%, only `number_of_facilities`, `actual_duration`, `nameAustria`, `nameBelgium`, `nameChina`, `nameMoldova, Republic of`, `nameUnited Kingdom` and `nameUnited States` pass the t test. For the countries variables, only `nameAustria` has significantly negative coefficient, which indicates a lower p value for their clinical trial outcomes. Before making further conclusions, we fit another two partial models two separate the effect of `number_of_facilities` and `actual_duration`.

```{r}
model <- lm(p_value ~ ., data = d |> select(-nct_id))
summary(model)
```

## Model with Countries Variables Only

In this part, we fit a linear regression model with countries variables only. Given confidence level 95%, only `nameMoldova, Republic of`, `nameUnited Kingdom` and `nameUnited States` pass the t test, and their coefficients are all positive. This indicates that clinical trials with the three countries involved tend to have less significant results.

```{r}
model <- lm(p_value ~ ., data = d |> select(-c(nct_id, number_of_facilities, actual_duration)))
summary(model)
```

## Model with no Countries Variables

In this part, we fit a model with no countries variable. From the summary table, we can see that the result is consistent with our full model. The result indicates that the clinical trials with more facilities involved and with shorter actual duration tend to generate significant outcomes.

```{r}
model <- lm(p_value ~ ., data = d |> select(p_value, number_of_facilities, actual_duration))
summary(model)
```
## Time Series Analysis

In this section, we delve into the time series analysis of our dataset. Our focus is to understand the trends and patterns over time in the clinical trial data.

```{r}
data(studies_all)
```

The first step involves loading the dataset `studies_all`. This dataset contains comprehensive information about various clinical trials, especially the time information, which is crucial for our analysis.

```{r}
studies_all_selected <- select(studies_all, nct_id, last_update_submitted_qc_date)

# Perform the inner join with d
d_longitudinal <- inner_join(d, studies_all_selected, by = "nct_id")
```

Here, we select the necessary columns from `studies_all`, specifically `nct_id` and 'last_update_submitted_qc_date'. Note that 'last_update_submitted_qc_date' contains the date when the last update or modification to a clinical trial's data or documentation was submitted for quality control (QC) review or assessment. It signifies a crucial timestamp in the clinical trial management process, indicating when the trial's records were last revised and prepared for quality assurance checks. We then perform an inner join with another dataset 'd'. We aim to merge relevant data from both datasets based on the `nct_id`, which is the unique identifier for each clinical trial. Now we will get time information for each study.

```{r}
any_na_in_data <- any(is.na(d_longitudinal))
print(any_na_in_data)
```

We then check for missing values in our merged dataset, `d_longitudinal`. The above code checks for any NA (Not Available) values in the dataset.

```{r}
d_longitudinal
```

The final step in our data preparation involves recovering the p_value column to the original value without any transformation. We apply a mathematical transformation to the p-values, converting them into their original scale. This step is crucial for making our subsequent statistical analysis more interpretable and meaningful.

```{r}
d_longitudinal <- d_longitudinal |> mutate(original_p_value = exp(p_value) / (1 + exp(p_value)))
```

In our analysis, we first converted 'last_update_submitted_qc_date' into a datetime format, essential for accurate time-series examination. Following this, we plotted the `original_p_value` against time to visually identify any significant trends. Our approach involved using a linear regression model, with time as our independent variable and `original_p_value` as the dependent one.

```{r}
# Convert dates to numerical values
d_longitudinal$last_update_submitted_qc_date <- as.Date(d_longitudinal$last_update_submitted_qc_date)
d_longitudinal$days_since_first <- as.numeric(d_longitudinal$last_update_submitted_qc_date - min(d_longitudinal$last_update_submitted_qc_date))

# Fit linear model
model <- lm(original_p_value ~ days_since_first, data = d_longitudinal)

# Summary of the model
summary(model)

# To plot the original_p_value over time
ggplot(d_longitudinal, aes(x = last_update_submitted_qc_date, y = original_p_value)) +
  geom_point(aes(color = "Original P-Value"), size = 2, alpha = 0.5) +
  geom_smooth(method = "lm", formula = y ~ x, color = "red", se = FALSE) +
  
  labs(x = "Date", y = "Original P-Value",
       title = "Original P-Value Over Time") +
  
  scale_color_manual(values = c("Original P-Value" = "blue")) +
  
  theme_minimal()
```

This model revealed a small yet statistically significant trend: a gradual increase in the `original_p_value` over time. It's important to note that the increase is modest, largely due to the inherently small size of the p-value. The exceptionally low p-value for the slope, significantly under $0.05$, validates the trend's statistical significance. However, the low coefficient of determination suggests that time only explains a minor part of the variance in the `original_p_value`. In summary, our analysis indicates a discernible, though minor, upward trend in the `original_p_value` over time. This trend, while statistically valid, contributes to only a small portion of the observed changes in our dataset.

This gradual increase in the `original_p_value` over time suggests that the studies we from 2010 to 2022 are showing a trend towards less statistically significant results as time progresses. This could imply a range of possibilities, such as changes in research methodology, variations in study quality, or shifts in the underlying phenomena being studied. It's a crucial insight for researchers and policy makers, as it highlights the need for a closer examination of why these changes are occurring. Understanding this trend could lead to more robust research practices and better-informed decision-making in the future.

Then we plot the decomposed components of our time series data, several insights emerge, further enhancing our understanding of the `original_p_value` trends over time.

```{r, fig.width=6, fig.height=6}
d_longitudinal$last_update_submitted_qc_date <- ymd(d_longitudinal$last_update_submitted_qc_date)
d_longitudinal <- d_longitudinal %>% arrange(last_update_submitted_qc_date)

# Group data by month and calculate the mean of `original_p_value`
monthly_data <- d_longitudinal %>%
  group_by(month = floor_date(last_update_submitted_qc_date, "month")) %>%
  summarize(original_p_value = mean(original_p_value, na.rm = TRUE))

# Convert to xts for decomposition
ts_data <- ts(monthly_data$original_p_value, frequency = 12)

# Decompose the time series (additive model)
decomposed_data <- decompose(ts_data, type = "additive")

# Plot the decomposed components
plot(decomposed_data)
```

**Trend Component Analysis**:
The trend component of the graph, which represents the long-term movement in the `original_p_value`, shows a small positive slope. This aligns with our earlier findings of a gradual increase in the p-value over time. The consistent yet modest upward trend in the p-values suggests a decrease in the statistical significance of the studies over the examined period. This could be indicative of evolving research dynamics, such as changes in research methodologies or variations in the domains being studied.

**Seasonal Component Insights**:
The seasonal component of the data reveals calendar-related fluctuations in the p-values. The volatility observed here is particularly intriguing. It suggests that there are specific times of the year when the studies tend to yield results with varying levels of significance. This could be due to a variety of factors, including cyclical research funding, academic calendars, or even seasonal variations in the phenomena being studied.

**Random Component (Noise) Considerations**:
The random component, which shows the residual fluctuations after accounting for trend and seasonal effects, appears erratic. This erratic nature indicates that there are additional factors influencing the p-values that our model has not accounted for. These could range from unforeseen external events impacting research outcomes to inherent randomness in the data collection processes.

# Interpretation and Conclusions

In this project, we aim to answer the question whether there are significant relations between the significance of the outcomes of clinical trials and factors such as countries that the trials are conducted and the number of facility involved. We conduct rich exploratory data analysis on our AACT dataset. The distribution of p value of the clinical trials is skewed toward 0, indicating that most clinical trials complete with a significant outcomes.In addition, our analysis on `number_of_facilities` indicates that the clinical trials have about 7 to 8 facilities involved on average. This also show the trend of collaboration in clinical trials. Furthermore, we fit three linear regression models and find that only United Kingdom and United States two countries have significant impact on the p values of clinical trials outcome and the clinical trials with the two countries involved tend to have less significant p values. Since the two countries get involve with a large portion of clinical trials, we interpret this result that it implies the fact that scientific discovery is a difficult process and tend to complete with a non-significant outcome. In addition, our model on `number_of_facilities` indicates that more collaboration can accelerate the process of scientific discovery. Thus there is not much significant relations between the significance of the outcomes of clinical trials and countries that the trials are conducted. But there is relations between the significance of the outcomes and the number of facility involved.

In addition to our primary findings, our time series analysis using a linear regression model unveiled a noteworthy trend: a gradual increase in `original_p_value` over time. This trend, while statistically significant with a p-value for the slope below 0.05, displayed a relatively modest effect size, as indicated by the low coefficient of determination. This observed trend implies several potential contributing factors, including shifts in research methodologies, variations in study quality, or changes in the phenomena under examination. These findings emphasize the importance of conducting further investigations to delve into the underlying causes, providing valuable insights for both researchers and policymakers. Furthermore, when we dissected the time series data, we gained additional insights. The trend component consistently exhibited a slight positive slope, reinforcing the notion of diminishing statistical significance over time. The seasonal component highlighted fluctuations in p-values linked to specific times of the year, potentially influenced by factors like research funding cycles, academic calendars, or seasonal variations in the phenomena being studied. Lastly, the random component pointed to erratic fluctuations, suggesting the presence of unaccounted factors impacting p-values, ranging from unforeseen external events to inherent variability in data collection practices.
